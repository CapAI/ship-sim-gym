{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ship Sim Gym\n",
    "\n",
    "Quick overview of training on a custom environment as it is slightly different from a normal one. \n",
    "\n",
    "Most cells contain code that is simply imported from corresponding py files. It is usually possible to call those scripts directly via their main hooks as well as long as you call them as modules `python -m ...`\n",
    "\n",
    "If you change something in the scripts themselves and would like to see the cell updated here as well, uncomment the first line in the cell that does the IPython magic called %load. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Ship Game\n",
    "\n",
    "This is a basic top down ship simulator built with pygame for the graphics and uses pymunk for physics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.4\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "Loading chipmunk for Darwin (64bit) [/Users/gerard/miniconda3/envs/ship-sim-gym-3.6/lib/python3.6/site-packages/pymunk/libchipmunk.dylib]\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ship_gym'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-def57ab7d3ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpymunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpygame_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mship_gym\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgame_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mship_gym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGameConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mship_gym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGameObject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mShip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPolyEnv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLiDAR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ship_gym'"
     ]
    }
   ],
   "source": [
    "# %load ../ship_gym/game.py\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pygame\n",
    "\n",
    "from pymunk import Vec2d, Transform\n",
    "import pymunk as pm\n",
    "import pymunk.pygame_util\n",
    "\n",
    "from ship_gym import game_map\n",
    "from ship_gym.config import GameConfig\n",
    "from ship_gym.models import GameObject, Ship, PolyEnv, LiDAR\n",
    "\n",
    "N_GOALS = 5\n",
    "DEFAULT_BOUNDS = (500, 500)\n",
    "\n",
    "\n",
    "class ShipGame(object):\n",
    "\n",
    "    ships = list()\n",
    "    goals = list()\n",
    "\n",
    "    frame_counter = 0\n",
    "    base_dt = 0.1\n",
    "    colliding = False\n",
    "    observe_mode = False\n",
    "    record = False\n",
    "\n",
    "    def __init__(self, game_config=None):\n",
    "\n",
    "        if game_config is None:\n",
    "            game_config = GameConfig\n",
    "\n",
    "        self.speed = game_config.SPEED\n",
    "        self.fps = game_config.FPS\n",
    "        self.bounds = game_config.BOUNDS\n",
    "        self.screen = pygame.display.set_mode(self.bounds)\n",
    "        self.clock = pygame.time.Clock()\n",
    "        self.goal_reached = False\n",
    "        self.colliding = False\n",
    "\n",
    "        self.debug_mode = game_config.DEBUG\n",
    "\n",
    "        pygame.init()\n",
    "        pygame.display.set_caption(\"Ship Sim Gym\")\n",
    "        pygame.key.set_repeat(10, 10)\n",
    "\n",
    "        print(\"-\"*30)\n",
    "        print(\"SHIP GAME INITIALIZED\")\n",
    "        print(\"DEBUG MODE = \", self.debug_mode)\n",
    "        print(\"GAME SPEED = \", self.speed)\n",
    "        print(\"GAME FPS   = \", self.fps)\n",
    "        print(\"-\"*30, \"\\n\")\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def gen_level(self):\n",
    "        \"\"\"\n",
    "        Generate a level on the fly by calling game map gen river poly function wrapping them in a GeoMap object\n",
    "        and adding the generated pymunk primitives (shapes and bodies) to the game space\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        poly = game_map.gen_river_poly(self.bounds)\n",
    "\n",
    "        self.level = PolyEnv(poly, self.bounds)\n",
    "\n",
    "        for body, shape in zip(self.level.bodies, self.level.shapes):\n",
    "            self.space.add(body, shape)\n",
    "\n",
    "    def invert_p(self, p):\n",
    "        \"\"\"Because in screen Y=0 is at the top or some shit like that \"\"\"\n",
    "        return Vec2d(p[0], self.bounds[1] - p[1])\n",
    "\n",
    "    def add_goal(self, x, y):\n",
    "        \"\"\"Add a ball to the given space at a random position \"\"\"\n",
    "        self.total_goals += 1\n",
    "\n",
    "        mass = 1\n",
    "        radius = 5\n",
    "        inertia = pm.moment_for_circle(mass, 0, radius, (0,0))\n",
    "        body = pm.Body(mass, inertia)\n",
    "\n",
    "        body.position = x, y\n",
    "        shape = pm.Circle(body, radius, (0,0))\n",
    "        shape.color = pygame.color.THECOLORS[\"green\"]\n",
    "        self.space.add(body, shape)\n",
    "        shape.collision_type = 2\n",
    "\n",
    "        goal = GameObject(body, shape)\n",
    "        self.goals.append(goal)\n",
    "\n",
    "        return goal\n",
    "\n",
    "    def add_player_ship(self, x, y, width, height, color):\n",
    "        \"\"\"\n",
    "        Call this after you have created the level!\n",
    "        Creates a new Ship instance and adds a shape and body to the pymunk space\n",
    "        :param self:\n",
    "        :param x:\n",
    "        :param y:\n",
    "        :param width:\n",
    "        :param height:\n",
    "        :param color:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        ship = Ship(x, y, width, height, color)\n",
    "        ship.add_lidar(self.level.shapes)\n",
    "\n",
    "        self.space.add(ship.body, ship.shape)\n",
    "\n",
    "        return ship\n",
    "\n",
    "    def add_ship(self, x, y, width, height, color):\n",
    "        \"\"\"\n",
    "        Creates a new Ship instance and adds a shape and body to the pymunk space\n",
    "        :param self:\n",
    "        :param x:\n",
    "        :param y:\n",
    "        :param width:\n",
    "        :param height:\n",
    "        :param color:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        ship = Ship(x, y, width, height, color)\n",
    "        self.space.add(ship.body, ship.shape)\n",
    "\n",
    "        return ship\n",
    "\n",
    "    def get_screen(self):\n",
    "        \"\"\"\n",
    "        Returns the game's screen space buffer as a 3D (color) array\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return pygame.surfarray.array3d(self.screen)\n",
    "\n",
    "    def handle_discrete_action(self, action):\n",
    "        \"\"\"\n",
    "        Handle discrete actions: It is possible to move forward, rotate left and right and do nothing.\n",
    "        Moving backwards is not possible, but is easy to add if needed. See the player definition in models for this.\n",
    "        :param action: integer value to indicate the action to take\n",
    "        \"\"\"\n",
    "        if action == 0:\n",
    "            self.player.move_forward()\n",
    "        elif action == 1:\n",
    "            self.player.rotate(-5)\n",
    "        elif action == 2:\n",
    "            self.player.rotate(+5)\n",
    "        elif action == 3:\n",
    "            pass\n",
    "\n",
    "    def handle_input(self):\n",
    "        \"\"\"\n",
    "        Maps key inputs to actions (via handle_discrete_action) and other utility functions such as quit\n",
    "        \"\"\"\n",
    "\n",
    "         # Handle key strokes\n",
    "        for event in pygame.event.get():\n",
    "\n",
    "            # print(event.key)\n",
    "            if event.type == pygame.QUIT:\n",
    "                sys.exit(0)\n",
    "\n",
    "            if event.type == pygame.KEYDOWN:\n",
    "                if event.key == pygame.K_ESCAPE or event.key == pygame.K_q:\n",
    "                    sys.exit(0)\n",
    "\n",
    "                elif event.key == pygame.K_w:\n",
    "                    self.handle_discrete_action(0)\n",
    "                    print(\"W pressed. \")\n",
    "                elif event.key == pygame.K_s:\n",
    "                    print(\"S pressed\")\n",
    "                    self.handle_discrete_action(1)\n",
    "                elif event.key == pygame.K_a:\n",
    "                    self.handle_discrete_action(2)\n",
    "                    print(\"A pressed. \", self.player.rudder_angle)\n",
    "                elif event.key == pygame.K_d:\n",
    "                    print(\"D pressed\")\n",
    "                    self.handle_discrete_action(3)\n",
    "\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\"\n",
    "        The main update loop, resets certain event states, handles input, sensor routines and updates the game's\n",
    "        pymunk space\n",
    "        \"\"\"\n",
    "        self.colliding = False\n",
    "        self.goal_reached = False\n",
    "        self.handle_input()\n",
    "        self.player.query_sensors()\n",
    "        self.space.step(self.speed * self.base_dt)\n",
    "        self.clock.tick(self.fps)\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        The main render loop clears the screen and draws primitives if requested\n",
    "        \"\"\"\n",
    "        self.screen.fill((0, 0, 200))\n",
    "        if self.debug_mode:\n",
    "            options = pm.pygame_util.DrawOptions(self.screen)\n",
    "            options.flags = pymunk.SpaceDebugDrawOptions.DRAW_SHAPES\n",
    "            self.space.debug_draw(options)\n",
    "\n",
    "            res = self.player.lidar.query_results\n",
    "            for r in res:\n",
    "                if r is not None and r.shape is None:\n",
    "                    p = r.point\n",
    "                    p = self.invert_p(p)\n",
    "                    p = (round(p.x), round(p.y))\n",
    "\n",
    "                    # Green circle indicating the rays did not hot anything\n",
    "                    pygame.draw.circle(self.screen, (0, 255, 0), p, 10)\n",
    "                else:\n",
    "                    p = r.point\n",
    "                    p = self.invert_p(p)\n",
    "                    p = (round(p.x), round(p.y))\n",
    "\n",
    "                    # Red circle\n",
    "                    pygame.draw.circle(self.screen, (255, 0, 0), p, 10)\n",
    "\n",
    "        p = self.invert_p(self.player.position)\n",
    "\n",
    "        pygame.draw.circle(self.screen, (255, 255, 0), (round(p.x), round(p.y)), 10)\n",
    "        pygame.display.flip()\n",
    "\n",
    "        self.frame_counter += 1\n",
    "\n",
    "\n",
    "    def collide_ship(self, arbiter, space, data):\n",
    "        \"\"\"\n",
    "        Ship collision callback for when the player ship hits another ship. All params are ignored at this point\n",
    "        :param arbiter:\n",
    "        :param space:\n",
    "        :param data:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.colliding = True\n",
    "        return True\n",
    "\n",
    "    def collide_goal(self, arbiter, space, data):\n",
    "        \"\"\"\n",
    "        Ship collision callback for when the player ship hits a goal object. All params are ignored at this point\n",
    "        :param arbiter:\n",
    "        :param space:\n",
    "        :param data:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        shape = arbiter.shapes[1]\n",
    "        space.remove(shape, shape.body)\n",
    "\n",
    "        self.goal_reached = True\n",
    "        self.goals = [g for g in self.goals if g.body is not shape.body]\n",
    "\n",
    "        return False\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the game. Create the environment, the player and the goals\n",
    "        :param spawn_point:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.total_goals = 0\n",
    "        self.ships = list()\n",
    "        self.goals = list()\n",
    "        self.space = pm.Space()\n",
    "        self.space.damping = 0.4\n",
    "        self.create_environment()\n",
    "        self.gen_goal_path(N_GOALS)\n",
    "\n",
    "        spawn_point = Vec2d(self.bounds[0] / 2, 25)\n",
    "        self.player = self.add_player_ship(spawn_point.x, spawn_point.y, 2, 3, pygame.color.THECOLORS[\"white\"])\n",
    "        self.player.shape.collision_type = 0\n",
    "        self.setup_collision_handlers()\n",
    "\n",
    "    def add_default_traffic(self):\n",
    "        \"\"\"\n",
    "        Add some simple static traffic to the game\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.ships.append(self.add_ship(100, 200, 1, 1, pygame.color.THECOLORS[\"black\"]))\n",
    "        self.ships.append(self.add_ship(300, 200, 1.5, 2, pygame.color.THECOLORS[\"black\"]))\n",
    "        self.ships.append(self.add_ship(400, 350, 1, 3, pygame.color.THECOLORS[\"black\"]))\n",
    "\n",
    "    def setup_collision_handlers(self):\n",
    "        \"\"\"\n",
    "        Add collision handlers to the game space for goal and obstacle interactions.\n",
    "        \"\"\"\n",
    "        h = self.space.add_collision_handler(0, 1)\n",
    "        h.begin = self.collide_ship\n",
    "\n",
    "        goal_agent_col = self.space.add_collision_handler(0, 2)\n",
    "        goal_agent_col.begin = self.collide_goal\n",
    "\n",
    "        self.space.add_collision_handler(0, 3)\n",
    "\n",
    "    def gen_goal_path(self, n):\n",
    "        \"\"\"\n",
    "        Generate a path of goals by sampling somewhat randomly the coordinate space. To avoid complete randomness\n",
    "        where it is hard to even see a path, I kind of use a jittery approach where delta_y is computed according to\n",
    "        the game bounds and incremented, and randomly jittered. The X position is determined by taking a jittered\n",
    "        point close to the midline and doing a segment query to the left and right on environmental level shapes (see\n",
    "        ShapeFilter). These points are then used as extreme points between which the X value is determined according\n",
    "        to some tolerance value.\n",
    "        :param n: number of goals to generate\n",
    "        \"\"\"\n",
    "\n",
    "        y_delta = self.bounds[1] / (n+1)\n",
    "        x_middle = self.bounds[0] / 2\n",
    "        x_jitter = 50\n",
    "        y_jitter = 20\n",
    "\n",
    "        tolerance = 60\n",
    "        filter = pymunk.ShapeFilter(mask=pymunk.ShapeFilter.ALL_MASKS ^ 0b1) # This has not been properly tested!\n",
    "\n",
    "        for i in range(1, n+1):\n",
    "            y = y_delta*i + random.randint(-y_jitter, y_jitter)\n",
    "            try:\n",
    "                left_ret = self.space.segment_query((self.bounds[0]/2, y), (0, y), 10, filter)[0]\n",
    "                right_ret = self.space.segment_query((self.bounds[0] / 2, y), (self.bounds[0], y), 10, filter)[0]\n",
    "\n",
    "                x = np.random.uniform(left_ret.point.x + tolerance, right_ret.point.x - tolerance)\n",
    "                self.add_goal(x, y)\n",
    "\n",
    "            except Exception as e:\n",
    "                x = x_middle * i + random.randint(-x_jitter, x_jitter)\n",
    "                self.add_goal(x, y)\n",
    "\n",
    "\n",
    "    def closest_goal(self):\n",
    "        \"\"\"\n",
    "        Return the goal with the smallest Euclidean distance to the player. Returns None if there are no goals left.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if len(self.goals):\n",
    "            min_goal = self.goals[0]\n",
    "            min_distance = min_goal.body.position.get_distance(self.player.body.position)\n",
    "            for goal in self.goals[1:]:\n",
    "\n",
    "                dist = goal.body.position.get_distance(self.player.body.position)\n",
    "                if dist < min_distance:\n",
    "                    min_distance = dist\n",
    "                    min_goal = goal\n",
    "\n",
    "            return min_goal\n",
    "        return None\n",
    "\n",
    "    def create_environment(self):\n",
    "        \"\"\"\n",
    "        The hook for creating the environment. Replace the call for gen_level\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.gen_level()\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    import os\n",
    "\n",
    "    cwd = os.getcwd()\n",
    "    gc = GameConfig\n",
    "    gc.SPEED = 1\n",
    "    gc.FPS = 30\n",
    "    gc.DEBUG = True\n",
    "\n",
    "    g = ShipGame()\n",
    "\n",
    "    while True:\n",
    "        g.update()\n",
    "        g.render()\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Gym Environment\n",
    "\n",
    "This creates a wrapper for the ship game I made\n",
    "\n",
    "The way you create an OpenAI gym is by defining a class that overrides the gym.Env class and override a few properties:\n",
    "\n",
    "```\n",
    "action_space\n",
    "observation_space\n",
    "```\n",
    "\n",
    "and a few functions:\n",
    "\n",
    "```\n",
    "step()\n",
    "reset()\n",
    "render()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment / Gym\n",
    "\n",
    "DEFAULT_STATE_VAL = -1\n",
    "STEP_PENALTY = -0.01\n",
    "\n",
    "class ShipEnv(Env):\n",
    "\n",
    "    metadata = {'render.modes': ['human', 'rgb_array']}\n",
    "    action_space = Discrete(5)\n",
    "    reward_range = (-1, 1)\n",
    "\n",
    "    # TODO: Derive the discrete actions\n",
    "    def __init__(self, game_config, env_config):\n",
    "\n",
    "        # TODO: Should add some basic sanity checks (max_steps > 0 etc.)\n",
    "        self.last_action = None\n",
    "        self.last_action = None\n",
    "        self.reward = 0\n",
    "        self.cumulative_reward = 0\n",
    "        self.step_count = 0\n",
    "        self.env_config = env_config\n",
    "\n",
    "        self.game = ShipGame(game_config)\n",
    "        self.episodes_count = -1 # Because the first reset will increment it to 0\n",
    "        self.n_states = 2 + 1 + 1 + 2 + self.game.player.lidar.n_beams\n",
    "        self.states_history = self.n_states * self.env_config.HISTORY_SIZE\n",
    "\n",
    "        if self.env_config.HISTORY_SIZE < 1:\n",
    "            raise ValueError(\"history_size must be greater than zero\")\n",
    "        self.observation_space = Box(low=0, high=max(self.game.bounds), shape=(self.states_history,), dtype=np.uint8)\n",
    "\n",
    "        # print(\" *** SHIP-GYM INITIALIZED *** \")\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        \"\"\"\n",
    "        Small but extremely important function, this makes sure that every environment you create is slightly different\n",
    "        otherwise parallelization is useless since the states will be exactly the same!\n",
    "        \"\"\"\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        np.random.seed(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def determine_reward(self):\n",
    "\n",
    "        if self.game.colliding:\n",
    "            self.reward = -1.0\n",
    "        if self.game.goal_reached:\n",
    "            self.reward = 1.0\n",
    "\n",
    "        # TODO: Code duplication with is_done()\n",
    "        elif self.game.player.x < 0 or self.game.player.x > self.game.bounds[0]:\n",
    "            self.reward = -1\n",
    "        elif self.game.player.y < 0 or self.game.player.y > self.game.bounds[1]:\n",
    "            self.reward = -1\n",
    "        else:\n",
    "            self.reward = STEP_PENALTY  # Small penalty\n",
    "\n",
    "    def _normalized_coords(self, x, y):\n",
    "        return x / self.game.bounds[0], y / self.game.bounds[1]\n",
    "\n",
    "    def __add_states(self):\n",
    "        '''\n",
    "        Push back some new state information for the current timestep onto the FIFO queue for all history timesteps\n",
    "        it keeps track of.\n",
    "\n",
    "        Layout of a single time step state is like this:\n",
    "\n",
    "        Px Py R Gx Gy L1 L2 ... Ln\n",
    "\n",
    "        Where\n",
    "        P is the player position\n",
    "        A is the player angle\n",
    "        R is the rudder angle\n",
    "        G is the nearest goal position\n",
    "        L are the lidar values\n",
    "        N is the number of rays lidar uses\n",
    "\n",
    "\n",
    "        :return: the complete history buffer of states extended with the most recent one\n",
    "        '''\n",
    "\n",
    "        states = self.n_states * [-1]\n",
    "        goal = self.game.closest_goal()\n",
    "        goal_pos = [-1, -1]\n",
    "        player = self.game.player\n",
    "\n",
    "        if goal:\n",
    "            goal_pos = [goal.body.position.x, goal.body.position.y]\n",
    "        states[:6] = [player.x, player.y, player.rudder_angle, player.body.angle, goal_pos[0], goal_pos[1]]\n",
    "\n",
    "        lidar_vals = self.game.player.lidar.vals\n",
    "\n",
    "        states[6:] = lidar_vals\n",
    "        self.states.extend(states)\n",
    "\n",
    "    def is_done(self):\n",
    "        if self.game.colliding:\n",
    "            # print(\"OOPS --- COLLISION\")\n",
    "            return True\n",
    "        elif len(self.game.goals) == 0:\n",
    "            print(\"ALL GOALS REACHED! -- CUMULATIVE REWARD = \", self.cumulative_reward)\n",
    "            return True\n",
    "\n",
    "        player = self.game.player\n",
    "        if player.x < 0 or player.x > self.game.bounds[0]:\n",
    "            print(\"X out of bounds\")\n",
    "            return True\n",
    "        elif player.y < 0 or player.y > self.game.bounds[1]:\n",
    "            print(\"Y out of bounds\")\n",
    "            return True\n",
    "\n",
    "        if self.step_count >= self.env_config.MAX_STEPS:\n",
    "            print(\"MAX STEPS\")\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action), \"%r (%s) invalid\" % (action, type(action))\n",
    "\n",
    "        self.game.handle_action(action)\n",
    "        self.game.update()\n",
    "        self.game.render()\n",
    "\n",
    "        self.determine_reward()\n",
    "        self.cumulative_reward += self.reward\n",
    "        self.__add_states()\n",
    "        self.step_count += 1\n",
    "\n",
    "        done = self.is_done()\n",
    "\n",
    "        return np.array(self.states), self.reward, done, {}\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        \"\"\"\n",
    "        This could be a rendered frame or just some stats that are used for debugging\n",
    "        \"\"\"\n",
    "        out = sys.stdout\n",
    "\n",
    "        if self.last_action is not None:\n",
    "            out.write(f'action={self.last_action}, cum_reward={self.cumulative_reward}')\n",
    "\n",
    "        return\n",
    "\n",
    "    def reset(self):\n",
    "        self.game.reset()\n",
    "\n",
    "        self.last_action = None\n",
    "        self.reward = 0\n",
    "        self.cumulative_reward = 0\n",
    "        self.step_count = 0\n",
    "        self.episodes_count += 1\n",
    "\n",
    "        n = self.n_states * self.env_config.HISTORY_SIZE\n",
    "        self.states = deque([DEFAULT_STATE_VAL] * n, maxlen=n)\n",
    "        self.__add_states()\n",
    "\n",
    "        return np.array(self.states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_config = GameConfig\n",
    "game_config.FPS = 1000\n",
    "game_config.SPEED = 30\n",
    "game_config.BOUNDS = (1000, 1000)\n",
    "\n",
    "def make_env(rank, game_config, env_config, seed=0):\n",
    "        \"\"\"\n",
    "        Utility function for multiprocessed env.\n",
    "\n",
    "        :param n_goals:\n",
    "        :param env_id: (str) the environment ID\n",
    "        :param num_env: (int) the number of environment you wish to have in subprocesses\n",
    "        :param seed: (int) the inital seed for RNG\n",
    "        :param rank: (int) index of the subprocess\n",
    "        \"\"\"\n",
    "\n",
    "        def _init():\n",
    "            env_config = EnvConfig\n",
    "            env = ShipEnv(game_config, env_config)\n",
    "            return env\n",
    "\n",
    "        return _init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Agent\n",
    "\n",
    "A simple baseline to compare against is a random agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init game at speed =  10\n",
      "Init game at fps =  10\n",
      "Reward =  -0.01\n",
      "States =  [200.          25.           0.           0.         187.19246125\n",
      "  69.66666667  -1.          -1.          -1.          -1.\n",
      "  -1.          -1.          -1.          -1.          -1.\n",
      "  -1.         200.          25.         -10.           0.\n",
      " 187.19246125  69.66666667  -1.          -1.          -1.\n",
      "  -1.          -1.          -1.          -1.          -1.\n",
      "  -1.          -1.        ]\n",
      "Reward =  -0.01\n",
      "States =  [200.          25.         -10.           0.         187.19246125\n",
      "  69.66666667  -1.          -1.          -1.          -1.\n",
      "  -1.          -1.          -1.          -1.          -1.\n",
      "  -1.         200.          25.           0.           0.\n",
      " 187.19246125  69.66666667  -1.          -1.          -1.\n",
      "  -1.          -1.          -1.          -1.          -1.\n",
      "  -1.          -1.        ]\n",
      "Reward =  -0.01\n",
      "States =  [200.          25.           0.           0.         187.19246125\n",
      "  69.66666667  -1.          -1.          -1.          -1.\n",
      "  -1.          -1.          -1.          -1.          -1.\n",
      "  -1.         200.          25.           0.           0.\n",
      " 187.19246125  69.66666667  -1.          -1.          -1.\n",
      "  -1.          -1.          -1.          -1.          -1.\n",
      "  -1.          -1.        ]\n",
      "Reward =  -0.01\n",
      "States =  [200.          25.           0.           0.         187.19246125\n",
      "  69.66666667  -1.          -1.          -1.          -1.\n",
      "  -1.          -1.          -1.          -1.          -1.\n",
      "  -1.         200.          75.           0.           0.\n",
      " 187.19246125  69.66666667  -1.          -1.          -1.\n",
      "  -1.          -1.          -1.          -1.          -1.\n",
      "  -1.          -1.        ]\n",
      "Reward =  -0.01\n",
      "States =  [200.          75.           0.           0.         187.19246125\n",
      "  69.66666667  -1.          -1.          -1.          -1.\n",
      "  -1.          -1.          -1.          -1.          -1.\n",
      "  -1.         200.         145.           0.           0.\n",
      " 167.1116884  115.33333333  -1.          -1.          -1.\n",
      "  -1.          -1.          -1.          -1.          -1.\n",
      "  -1.          -1.        ]\n",
      "Reward =  -0.01\n",
      "States =  [200.         145.           0.           0.         167.1116884\n",
      " 115.33333333  -1.          -1.          -1.          -1.\n",
      "  -1.          -1.          -1.          -1.          -1.\n",
      "  -1.         200.         223.          10.           0.\n",
      " 168.88262996 184.          -1.          -1.          -1.\n",
      "  -1.          -1.          -1.          -1.          -1.\n",
      "  -1.          -1.        ]\n",
      "Reward =  -0.01\n",
      "States =  [200.         223.          10.           0.         168.88262996\n",
      " 184.          -1.          -1.          -1.          -1.\n",
      "  -1.          -1.          -1.          -1.          -1.\n",
      "  -1.         200.         254.2         10.           0.\n",
      " 228.73600137 271.66666667  -1.          -1.          -1.\n",
      "  -1.          -1.          -1.          -1.          -1.\n",
      "  -1.          -1.        ]\n",
      "Reward =  -0.01\n",
      "States =  [200.         254.2         10.           0.         228.73600137\n",
      " 271.66666667  -1.          -1.          -1.          -1.\n",
      "  -1.          -1.          -1.          -1.          -1.\n",
      "  -1.         200.         316.68         0.          -0.8097166\n",
      " 171.96918749 342.33333333  -1.          -1.          -1.\n",
      "  -1.          -1.          -1.          -1.          -1.\n",
      "  -1.          -1.        ]\n",
      "Reward =  -0.01\n",
      "States =  [200.         316.68         0.          -0.8097166  171.96918749\n",
      " 342.33333333  -1.          -1.          -1.          -1.\n",
      "  -1.          -1.          -1.          -1.          -1.\n",
      "  -1.         200.         341.672      -10.          -1.13360324\n",
      " 171.96918749 342.33333333  88.14454343  90.78113307  96.01748835\n",
      "  -1.          -1.          -1.          -1.          -1.\n",
      "  -1.          -1.        ]\n",
      "Reward =  -0.01\n",
      "States =  [200.         341.672      -10.          -1.13360324 171.96918749\n",
      " 342.33333333  88.14454343  90.78113307  96.01748835  -1.\n",
      "  -1.          -1.          -1.          -1.          -1.\n",
      "  -1.         200.         351.6688     -10.          -1.26315789\n",
      " 171.96918749 342.33333333  89.79533428  87.84236567  88.09557072\n",
      "  90.58724652  95.64768828  -1.          -1.          -1.\n",
      "  -1.          -1.        ]\n",
      "Reward =  -0.01\n",
      "States =  [200.         351.6688     -10.          -1.26315789 171.96918749\n",
      " 342.33333333  89.79533428  87.84236567  88.09557072  90.58724652\n",
      "  95.64768828  -1.          -1.          -1.          -1.\n",
      "  -1.         245.29718361 376.83743161 -10.          -0.50526316\n",
      " 171.96918749 342.33333333  93.95640069  89.97581526  88.40720749\n",
      "  89.04742812  91.9783213   97.59357897  -1.          -1.\n",
      "  -1.          -1.        ]\n",
      "Reward =  -0.01\n",
      "States =  [ 2.45297184e+02  3.76837432e+02 -1.00000000e+01 -5.05263158e-01\n",
      "  1.71969187e+02  3.42333333e+02  9.39564007e+01  8.99758153e+01\n",
      "  8.84072075e+01  8.90474281e+01  9.19783213e+01  9.75935790e+01\n",
      " -1.00000000e+00 -1.00000000e+00 -1.00000000e+00 -1.00000000e+00\n",
      "  2.63416057e+02  3.86904884e+02  0.00000000e+00 -2.02105263e-01\n",
      "  1.71969187e+02  3.42333333e+02  5.96369730e+01  6.48573375e+01\n",
      "  7.30506242e+01  8.90474281e+01  9.19783213e+01  9.75935790e+01\n",
      " -1.00000000e+00 -1.00000000e+00 -1.00000000e+00 -1.00000000e+00]\n",
      "Reward =  -0.01\n",
      "States =  [ 2.63416057e+02  3.86904884e+02  0.00000000e+00 -2.02105263e-01\n",
      "  1.71969187e+02  3.42333333e+02  5.96369730e+01  6.48573375e+01\n",
      "  7.30506242e+01  8.90474281e+01  9.19783213e+01  9.75935790e+01\n",
      " -1.00000000e+00 -1.00000000e+00 -1.00000000e+00 -1.00000000e+00\n",
      "  2.70663606e+02  3.90931865e+02  0.00000000e+00 -8.08421053e-02\n",
      "  1.71969187e+02  3.42333333e+02  5.46802661e+01  6.48573375e+01\n",
      "  7.30506242e+01  8.90474281e+01  9.19783213e+01  9.75935790e+01\n",
      " -1.00000000e+00 -1.00000000e+00 -1.00000000e+00 -1.00000000e+00]\n",
      "Reward =  -0.01\n",
      "States =  [ 2.70663606e+02  3.90931865e+02  0.00000000e+00 -8.08421053e-02\n",
      "  1.71969187e+02  3.42333333e+02  5.46802661e+01  6.48573375e+01\n",
      "  7.30506242e+01  8.90474281e+01  9.19783213e+01  9.75935790e+01\n",
      " -1.00000000e+00 -1.00000000e+00 -1.00000000e+00 -1.00000000e+00\n",
      "  2.73562626e+02  3.92542658e+02  0.00000000e+00 -3.23368421e-02\n",
      "  1.71969187e+02  3.42333333e+02  5.46802661e+01  6.48573375e+01\n",
      "  7.30506242e+01  8.90474281e+01  9.19783213e+01  9.75935790e+01\n",
      " -1.00000000e+00 -1.00000000e+00 -1.00000000e+00 -1.00000000e+00]\n",
      "Y out of bounds\n",
      "Reward =  -1\n",
      "States =  [ 2.73562626e+02  3.92542658e+02  0.00000000e+00 -3.23368421e-02\n",
      "  1.71969187e+02  3.42333333e+02  5.46802661e+01  6.48573375e+01\n",
      "  7.30506242e+01  8.90474281e+01  9.19783213e+01  9.75935790e+01\n",
      " -1.00000000e+00 -1.00000000e+00 -1.00000000e+00 -1.00000000e+00\n",
      "  2.78759938e+02  4.43023678e+02  0.00000000e+00 -1.29347368e-02\n",
      "  1.71969187e+02  3.42333333e+02  5.46802661e+01  6.48573375e+01\n",
      "  7.30506242e+01  8.90474281e+01  9.19783213e+01  9.75935790e+01\n",
      " -1.00000000e+00 -1.00000000e+00 -1.00000000e+00 -1.00000000e+00]\n",
      "AGENT IS DONE. TOTAL REWARD = -1.14\n"
     ]
    }
   ],
   "source": [
    "gc = GameConfig\n",
    "gc.FPS = 10\n",
    "gc.SPEED = 10\n",
    "gc.BOUNDS = (400,400)\n",
    "\n",
    "env = ShipEnv(game_config=gc, env_config=EnvConfig)\n",
    "env.reset()\n",
    "\n",
    "rewards = list()\n",
    "\n",
    "for _ in range(1):\n",
    "\n",
    "    episode_reward = 0\n",
    "    for _ in range(1000):\n",
    "        env.render()\n",
    "\n",
    "        states, reward, done, _ = env.step(env.action_space.sample()) # take a random action\n",
    "        # ret = env.step(0) # take a random action\n",
    "\n",
    "        episode_reward += reward\n",
    "        \n",
    "        print(\"Reward = \", reward)\n",
    "        print(\"States = \", states)\n",
    "        \n",
    "        if done == True:\n",
    "            print(f\"AGENT IS DONE. TOTAL REWARD = {episode_reward}\")\n",
    "            rewards.append(episode_reward)\n",
    "            env.reset()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'register_env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-7ddda8eff3f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mregister_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ShipGym-v1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_creator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m pbt = PopulationBasedTraining(\n",
      "\u001b[0;31mNameError\u001b[0m: name 'register_env' is not defined"
     ]
    }
   ],
   "source": [
    "game_config = GameConfig\n",
    "game_config.FPS = 1000\n",
    "game_config.SPEED = 30\n",
    "game_config.BOUNDS = (1000, 1000)\n",
    "\n",
    "def env_creator(env_config):\n",
    "\n",
    "    env_config = EnvConfig\n",
    "    env = ShipEnv(game_config, env_config)\n",
    "\n",
    "    return env\n",
    "\n",
    "register_env(\"ShipGym-v1\", env_creator)\n",
    "\n",
    "pbt = PopulationBasedTraining(\n",
    "time_attr=\"time_total_s\",\n",
    "reward_attr=\"episode_reward_mean\",\n",
    "perturbation_interval=300, # 5 mins\n",
    "resample_probability=0.25,\n",
    "\n",
    "# Specifies the mutations of these hyperparams\n",
    "hyperparam_mutations={\n",
    "    \"lambda\": lambda: random.uniform(0.9, 1.0),\n",
    "    \"clip_param\": lambda: random.uniform(0.01, 0.5),\n",
    "    \"lr\": [1e-3, 5e-4, 1e-4, 5e-5, 1e-5],\n",
    "    \"num_sgd_iter\": lambda: random.randint(1, 30),\n",
    "    \"sgd_minibatch_size\": lambda: random.randint(128, 16384),\n",
    "    \"train_batch_size\": lambda: random.randint(2000, 160000),\n",
    "})\n",
    "\n",
    "ray.init()\n",
    "\n",
    "n_goals = 5\n",
    "reward_done = .9*n_goals\n",
    "\n",
    "run_experiments(\n",
    "{\n",
    "    \"pbt_ship_sim\": {\n",
    "        \"run\": \"PPO\",\n",
    "        \"env\": \"ShipGym-v1\",\n",
    "        \"num_samples\": 12, # Repeat the experiment this many times\n",
    "        \"checkpoint_at_end\" : True,\n",
    "        \"checkpoint_freq\" : 10,\n",
    "        \"config\": {\n",
    "            \"kl_coeff\": 1.0,\n",
    "            \"num_workers\": multiprocessing.cpu_count() - 1,\n",
    "            \"num_gpus\": 1,\n",
    "            \n",
    "            # These params are tuned from a fixed starting value.\n",
    "            \"lambda\": 0.95,\n",
    "            \"clip_param\": 0.2,\n",
    "            \"lr\" : 5.0e-4,\n",
    "            \"num_sgd_iter\":\n",
    "                lambda spec: random.choice([10, 20, 30]),\n",
    "            \"sgd_minibatch_size\":\n",
    "                lambda spec: random.choice([128, 512, 2048]),\n",
    "            \"train_batch_size\":\n",
    "                lambda spec: random.choice([10000, 20000, 40000])\n",
    "        },\n",
    "    },\n",
    "}, scheduler=pbt) # Reference the scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Visualisation\n",
    "\n",
    "rllib has very nice Tensorboard integration.\n",
    "\n",
    "[http://localhost:6006/](http://localhost:6006/)\n",
    "\n",
    "Insert TensorBoard image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate\n",
    "\n",
    "Let's look at some models we have saved, change the res_dir to whatever directory you are using to store the rllib log data and models in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "res_dir = \"/Users/gerard/Desktop/pbt_ship_sim_v2\"\n",
    "\n",
    "def read_csvs():\n",
    "    result_csvs = glob.glob(os.path.join(res_dir, \"*/progress.csv\"))\n",
    "\n",
    "    print(f\"{len(result_csvs)} CSV files found!\")\n",
    "    dfs = []\n",
    "\n",
    "    for res_csv in result_csvs:\n",
    "\n",
    "        log_dir = res_csv.split(\"/\")[-2]\n",
    "\n",
    "        df_res = pd.read_csv(res_csv, delimiter=\",\", header=0, quotechar='\"', quoting=csv.QUOTE_ALL)\n",
    "        df_res['log_dir'] = log_dir\n",
    "        dfs.append(df_res)\n",
    "\n",
    "    frame = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "\n",
    "# params = [json.load(open(p)) for p in glob.glob(os.path.join(res_dir, \"*/params.json\"))]\n",
    "# results = [json.load(open(p)) for p in glob.glob(os.path.join(res_dir, \"*/result.json\"))]\n",
    "\n",
    "\n",
    "\n",
    "paths = list()\n",
    "mean_rewards = list()\n",
    "\n",
    "cols = []\n",
    "\n",
    "for res in glob.glob(os.path.join(res_dir, \"*/result.json\")):\n",
    "    \n",
    "    try:\n",
    "        f = open(res)\n",
    "        for line in f:\n",
    "            j = json.loads(line)\n",
    "\n",
    "            path = \"/\".join(res.split('/')[:-1])\n",
    "            paths.append(path)\n",
    "        \n",
    "            \n",
    "            mean_rewards.append(j['episode_reward_mean'])\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(\"Could not read\")\n",
    "        print(e)\n",
    "        \n",
    "df = pd.DataFrame({'mean_reward':mean_rewards, 'path':paths})\n",
    "df.sort_values(by='mean_reward', ascending=False, inplace=True)\n",
    "\n",
    "# pp.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/gerard/Desktop/pbt_ship_sim_v2/PPO_ShipGym-v1_1_num_sgd_iter=30,sgd_minibatch_size=2048,train_batch_size=40000_2018-12-05_17-41-41woi3j510'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['path'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ship-sim-gym3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
