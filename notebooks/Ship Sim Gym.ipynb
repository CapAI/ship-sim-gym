{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ship Sim Gym\n",
    "\n",
    "Quick overview of training on a custom environment as it is slightly different from a normal one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading chipmunk for Darwin (64bit) [/Users/gerard/miniconda3/envs/ship-sim-gym-3.6/lib/python3.6/site-packages/pymunk/libchipmunk.dylib]\n",
      "pygame 1.9.4\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# This makes sure we have access to the top level directory and so the containing modules\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "\n",
    "from ship_gym.config import EnvConfig, GameConfig\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "from gym.utils import seeding\n",
    "\n",
    "from pymunk import Vec2d\n",
    "\n",
    "from ship_gym.curriculum import Curriculum\n",
    "from ship_gym.game import ShipGame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Ship Game\n",
    "\n",
    "This is a basic top down ship simulator built with pygame for the graphics and uses pymunk for physics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init game at speed =  1\n",
      "Init game at fps =  30\n"
     ]
    }
   ],
   "source": [
    "# The Game\n",
    "# This doesn't do so well in Jupyter notebook. If it gives you issues run it from commandline\n",
    "\n",
    "%run ../ship_gym/game.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Gym Environment\n",
    "\n",
    "This creates a wrapper for the ship game I made\n",
    "\n",
    "The way you create an OpenAI gym is by defining a class that overrides the gym.Env class and override a few properties:\n",
    "\n",
    "```\n",
    "action_space\n",
    "observation_space\n",
    "```\n",
    "\n",
    "and a few functions:\n",
    "\n",
    "```\n",
    "step()\n",
    "reset()\n",
    "render()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment / Gym\n",
    "\n",
    "DEFAULT_STATE_VAL = -1\n",
    "STEP_PENALTY = -0.01\n",
    "\n",
    "class ShipEnv(Env):\n",
    "\n",
    "    metadata = {'render.modes': ['human', 'rgb_array']}\n",
    "    action_space = Discrete(5)\n",
    "    reward_range = (-1, 1)\n",
    "\n",
    "    # TODO: Derive the discrete actions\n",
    "    def __init__(self, game_config, env_config):\n",
    "\n",
    "        # TODO: Should add some basic sanity checks (max_steps > 0 etc.)\n",
    "        self.last_action = None\n",
    "        self.last_action = None\n",
    "        self.reward = 0\n",
    "        self.cumulative_reward = 0\n",
    "        self.step_count = 0\n",
    "        self.env_config = env_config\n",
    "\n",
    "        self.game = ShipGame(game_config)\n",
    "        self.episodes_count = -1 # Because the first reset will increment it to 0\n",
    "        self.n_states = 2 + 1 + 1 + 2 + self.game.player.lidar.n_beams\n",
    "        self.states_history = self.n_states * self.env_config.HISTORY_SIZE\n",
    "\n",
    "        if self.env_config.HISTORY_SIZE < 1:\n",
    "            raise ValueError(\"history_size must be greater than zero\")\n",
    "        self.observation_space = Box(low=0, high=max(self.game.bounds), shape=(self.states_history,), dtype=np.uint8)\n",
    "\n",
    "        # print(\" *** SHIP-GYM INITIALIZED *** \")\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        \"\"\"\n",
    "        Small but extremely important function, this makes sure that every environment you create is slightly different\n",
    "        otherwise parallelization is useless since the states will be exactly the same!\n",
    "        \"\"\"\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        np.random.seed(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def determine_reward(self):\n",
    "\n",
    "        if self.game.colliding:\n",
    "            self.reward = -1.0\n",
    "        if self.game.goal_reached:\n",
    "            self.reward = 1.0\n",
    "\n",
    "        # TODO: Code duplication with is_done()\n",
    "        elif self.game.player.x < 0 or self.game.player.x > self.game.bounds[0]:\n",
    "            self.reward = -1\n",
    "        elif self.game.player.y < 0 or self.game.player.y > self.game.bounds[1]:\n",
    "            self.reward = -1\n",
    "        else:\n",
    "            self.reward = STEP_PENALTY  # Small penalty\n",
    "\n",
    "    def _normalized_coords(self, x, y):\n",
    "        return x / self.game.bounds[0], y / self.game.bounds[1]\n",
    "\n",
    "    def __add_states(self):\n",
    "        '''\n",
    "        Push back some new state information for the current timestep onto the FIFO queue for all history timesteps\n",
    "        it keeps track of.\n",
    "\n",
    "        Layout of a single time step state is like this:\n",
    "\n",
    "        Px Py R Gx Gy L1 L2 ... Ln\n",
    "\n",
    "        Where\n",
    "        P is the player position\n",
    "        A is the player angle\n",
    "        R is the rudder angle\n",
    "        G is the nearest goal position\n",
    "        L are the lidar values\n",
    "        N is the number of rays lidar uses\n",
    "\n",
    "\n",
    "        :return: the complete history buffer of states extended with the most recent one\n",
    "        '''\n",
    "\n",
    "        states = self.n_states * [-1]\n",
    "        goal = self.game.closest_goal()\n",
    "        goal_pos = [-1, -1]\n",
    "        player = self.game.player\n",
    "\n",
    "        if goal:\n",
    "            goal_pos = [goal.body.position.x, goal.body.position.y]\n",
    "        states[:6] = [player.x, player.y, player.rudder_angle, player.body.angle, goal_pos[0], goal_pos[1]]\n",
    "\n",
    "        lidar_vals = self.game.player.lidar.vals\n",
    "\n",
    "        states[6:] = lidar_vals\n",
    "        self.states.extend(states)\n",
    "\n",
    "    def is_done(self):\n",
    "        if self.game.colliding:\n",
    "            # print(\"OOPS --- COLLISION\")\n",
    "            return True\n",
    "        elif len(self.game.goals) == 0:\n",
    "            print(\"ALL GOALS REACHED! -- CUMULATIVE REWARD = \", self.cumulative_reward)\n",
    "            return True\n",
    "\n",
    "        player = self.game.player\n",
    "        if player.x < 0 or player.x > self.game.bounds[0]:\n",
    "            print(\"X out of bounds\")\n",
    "            return True\n",
    "        elif player.y < 0 or player.y > self.game.bounds[1]:\n",
    "            print(\"Y out of bounds\")\n",
    "            return True\n",
    "\n",
    "        if self.step_count >= self.env_config.MAX_STEPS:\n",
    "            print(\"MAX STEPS\")\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action), \"%r (%s) invalid\" % (action, type(action))\n",
    "\n",
    "        self.game.handle_action(action)\n",
    "        self.game.update()\n",
    "        self.game.render()\n",
    "\n",
    "        self.determine_reward()\n",
    "        self.cumulative_reward += self.reward\n",
    "        self.__add_states()\n",
    "        self.step_count += 1\n",
    "\n",
    "        done = self.is_done()\n",
    "\n",
    "        return np.array(self.states), self.reward, done, {}\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        \"\"\"\n",
    "        This could be a rendered frame or just some stats that are used for debugging\n",
    "        \"\"\"\n",
    "        out = sys.stdout\n",
    "\n",
    "        if self.last_action is not None:\n",
    "            out.write(f'action={self.last_action}, cum_reward={self.cumulative_reward}')\n",
    "\n",
    "        return\n",
    "\n",
    "    def reset(self):\n",
    "        self.game.reset()\n",
    "\n",
    "        self.last_action = None\n",
    "        self.reward = 0\n",
    "        self.cumulative_reward = 0\n",
    "        self.step_count = 0\n",
    "        self.episodes_count += 1\n",
    "\n",
    "        n = self.n_states * self.env_config.HISTORY_SIZE\n",
    "        self.states = deque([DEFAULT_STATE_VAL] * n, maxlen=n)\n",
    "        self.__add_states()\n",
    "\n",
    "        return np.array(self.states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_config = GameConfig\n",
    "game_config.FPS = 1000\n",
    "game_config.SPEED = 30\n",
    "game_config.BOUNDS = (1000, 1000)\n",
    "\n",
    "def make_env(rank, game_config, env_config, seed=0):\n",
    "        \"\"\"\n",
    "        Utility function for multiprocessed env.\n",
    "\n",
    "        :param n_goals:\n",
    "        :param env_id: (str) the environment ID\n",
    "        :param num_env: (int) the number of environment you wish to have in subprocesses\n",
    "        :param seed: (int) the inital seed for RNG\n",
    "        :param rank: (int) index of the subprocess\n",
    "        \"\"\"\n",
    "\n",
    "        def _init():\n",
    "            env_config = EnvConfig\n",
    "            env = ShipEnv(game_config, env_config)\n",
    "            return env\n",
    "\n",
    "        return _init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Agent\n",
    "\n",
    "A simple baseline to compare against is a random agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init game at speed =  10\n",
      "Init game at fps =  10\n",
      "Reward =  -0.01\n",
      "States =  [200.          25.           0.           0.         187.19246125\n",
      "  69.66666667  -1.          -1.          -1.          -1.\n",
      "  -1.          -1.          -1.          -1.          -1.\n",
      "  -1.         200.          25.         -10.           0.\n",
      " 187.19246125  69.66666667  -1.          -1.          -1.\n",
      "  -1.          -1.          -1.          -1.          -1.\n",
      "  -1.          -1.        ]\n",
      "Reward =  -0.01\n",
      "States =  [200.          25.         -10.           0.         187.19246125\n",
      "  69.66666667  -1.          -1.          -1.          -1.\n",
      "  -1.          -1.          -1.          -1.          -1.\n",
      "  -1.         200.          25.           0.           0.\n",
      " 187.19246125  69.66666667  -1.          -1.          -1.\n",
      "  -1.          -1.          -1.          -1.          -1.\n",
      "  -1.          -1.        ]\n",
      "Reward =  -0.01\n",
      "States =  [200.          25.           0.           0.         187.19246125\n",
      "  69.66666667  -1.          -1.          -1.          -1.\n",
      "  -1.          -1.          -1.          -1.          -1.\n",
      "  -1.         200.          25.           0.           0.\n",
      " 187.19246125  69.66666667  -1.          -1.          -1.\n",
      "  -1.          -1.          -1.          -1.          -1.\n",
      "  -1.          -1.        ]\n",
      "Reward =  -0.01\n",
      "States =  [200.          25.           0.           0.         187.19246125\n",
      "  69.66666667  -1.          -1.          -1.          -1.\n",
      "  -1.          -1.          -1.          -1.          -1.\n",
      "  -1.         200.          75.           0.           0.\n",
      " 187.19246125  69.66666667  -1.          -1.          -1.\n",
      "  -1.          -1.          -1.          -1.          -1.\n",
      "  -1.          -1.        ]\n",
      "Reward =  -0.01\n",
      "States =  [200.          75.           0.           0.         187.19246125\n",
      "  69.66666667  -1.          -1.          -1.          -1.\n",
      "  -1.          -1.          -1.          -1.          -1.\n",
      "  -1.         200.         145.           0.           0.\n",
      " 167.1116884  115.33333333  -1.          -1.          -1.\n",
      "  -1.          -1.          -1.          -1.          -1.\n",
      "  -1.          -1.        ]\n",
      "Reward =  -0.01\n",
      "States =  [200.         145.           0.           0.         167.1116884\n",
      " 115.33333333  -1.          -1.          -1.          -1.\n",
      "  -1.          -1.          -1.          -1.          -1.\n",
      "  -1.         200.         223.          10.           0.\n",
      " 168.88262996 184.          -1.          -1.          -1.\n",
      "  -1.          -1.          -1.          -1.          -1.\n",
      "  -1.          -1.        ]\n",
      "Reward =  -0.01\n",
      "States =  [200.         223.          10.           0.         168.88262996\n",
      " 184.          -1.          -1.          -1.          -1.\n",
      "  -1.          -1.          -1.          -1.          -1.\n",
      "  -1.         200.         254.2         10.           0.\n",
      " 228.73600137 271.66666667  -1.          -1.          -1.\n",
      "  -1.          -1.          -1.          -1.          -1.\n",
      "  -1.          -1.        ]\n",
      "Reward =  -0.01\n",
      "States =  [200.         254.2         10.           0.         228.73600137\n",
      " 271.66666667  -1.          -1.          -1.          -1.\n",
      "  -1.          -1.          -1.          -1.          -1.\n",
      "  -1.         200.         316.68         0.          -0.8097166\n",
      " 171.96918749 342.33333333  -1.          -1.          -1.\n",
      "  -1.          -1.          -1.          -1.          -1.\n",
      "  -1.          -1.        ]\n",
      "Reward =  -0.01\n",
      "States =  [200.         316.68         0.          -0.8097166  171.96918749\n",
      " 342.33333333  -1.          -1.          -1.          -1.\n",
      "  -1.          -1.          -1.          -1.          -1.\n",
      "  -1.         200.         341.672      -10.          -1.13360324\n",
      " 171.96918749 342.33333333  88.14454343  90.78113307  96.01748835\n",
      "  -1.          -1.          -1.          -1.          -1.\n",
      "  -1.          -1.        ]\n",
      "Reward =  -0.01\n",
      "States =  [200.         341.672      -10.          -1.13360324 171.96918749\n",
      " 342.33333333  88.14454343  90.78113307  96.01748835  -1.\n",
      "  -1.          -1.          -1.          -1.          -1.\n",
      "  -1.         200.         351.6688     -10.          -1.26315789\n",
      " 171.96918749 342.33333333  89.79533428  87.84236567  88.09557072\n",
      "  90.58724652  95.64768828  -1.          -1.          -1.\n",
      "  -1.          -1.        ]\n",
      "Reward =  -0.01\n",
      "States =  [200.         351.6688     -10.          -1.26315789 171.96918749\n",
      " 342.33333333  89.79533428  87.84236567  88.09557072  90.58724652\n",
      "  95.64768828  -1.          -1.          -1.          -1.\n",
      "  -1.         245.29718361 376.83743161 -10.          -0.50526316\n",
      " 171.96918749 342.33333333  93.95640069  89.97581526  88.40720749\n",
      "  89.04742812  91.9783213   97.59357897  -1.          -1.\n",
      "  -1.          -1.        ]\n",
      "Reward =  -0.01\n",
      "States =  [ 2.45297184e+02  3.76837432e+02 -1.00000000e+01 -5.05263158e-01\n",
      "  1.71969187e+02  3.42333333e+02  9.39564007e+01  8.99758153e+01\n",
      "  8.84072075e+01  8.90474281e+01  9.19783213e+01  9.75935790e+01\n",
      " -1.00000000e+00 -1.00000000e+00 -1.00000000e+00 -1.00000000e+00\n",
      "  2.63416057e+02  3.86904884e+02  0.00000000e+00 -2.02105263e-01\n",
      "  1.71969187e+02  3.42333333e+02  5.96369730e+01  6.48573375e+01\n",
      "  7.30506242e+01  8.90474281e+01  9.19783213e+01  9.75935790e+01\n",
      " -1.00000000e+00 -1.00000000e+00 -1.00000000e+00 -1.00000000e+00]\n",
      "Reward =  -0.01\n",
      "States =  [ 2.63416057e+02  3.86904884e+02  0.00000000e+00 -2.02105263e-01\n",
      "  1.71969187e+02  3.42333333e+02  5.96369730e+01  6.48573375e+01\n",
      "  7.30506242e+01  8.90474281e+01  9.19783213e+01  9.75935790e+01\n",
      " -1.00000000e+00 -1.00000000e+00 -1.00000000e+00 -1.00000000e+00\n",
      "  2.70663606e+02  3.90931865e+02  0.00000000e+00 -8.08421053e-02\n",
      "  1.71969187e+02  3.42333333e+02  5.46802661e+01  6.48573375e+01\n",
      "  7.30506242e+01  8.90474281e+01  9.19783213e+01  9.75935790e+01\n",
      " -1.00000000e+00 -1.00000000e+00 -1.00000000e+00 -1.00000000e+00]\n",
      "Reward =  -0.01\n",
      "States =  [ 2.70663606e+02  3.90931865e+02  0.00000000e+00 -8.08421053e-02\n",
      "  1.71969187e+02  3.42333333e+02  5.46802661e+01  6.48573375e+01\n",
      "  7.30506242e+01  8.90474281e+01  9.19783213e+01  9.75935790e+01\n",
      " -1.00000000e+00 -1.00000000e+00 -1.00000000e+00 -1.00000000e+00\n",
      "  2.73562626e+02  3.92542658e+02  0.00000000e+00 -3.23368421e-02\n",
      "  1.71969187e+02  3.42333333e+02  5.46802661e+01  6.48573375e+01\n",
      "  7.30506242e+01  8.90474281e+01  9.19783213e+01  9.75935790e+01\n",
      " -1.00000000e+00 -1.00000000e+00 -1.00000000e+00 -1.00000000e+00]\n",
      "Y out of bounds\n",
      "Reward =  -1\n",
      "States =  [ 2.73562626e+02  3.92542658e+02  0.00000000e+00 -3.23368421e-02\n",
      "  1.71969187e+02  3.42333333e+02  5.46802661e+01  6.48573375e+01\n",
      "  7.30506242e+01  8.90474281e+01  9.19783213e+01  9.75935790e+01\n",
      " -1.00000000e+00 -1.00000000e+00 -1.00000000e+00 -1.00000000e+00\n",
      "  2.78759938e+02  4.43023678e+02  0.00000000e+00 -1.29347368e-02\n",
      "  1.71969187e+02  3.42333333e+02  5.46802661e+01  6.48573375e+01\n",
      "  7.30506242e+01  8.90474281e+01  9.19783213e+01  9.75935790e+01\n",
      " -1.00000000e+00 -1.00000000e+00 -1.00000000e+00 -1.00000000e+00]\n",
      "AGENT IS DONE. TOTAL REWARD = -1.14\n"
     ]
    }
   ],
   "source": [
    "gc = GameConfig\n",
    "gc.FPS = 10\n",
    "gc.SPEED = 10\n",
    "gc.BOUNDS = (400,400)\n",
    "\n",
    "env = ShipEnv(game_config=gc, env_config=EnvConfig)\n",
    "env.reset()\n",
    "\n",
    "rewards = list()\n",
    "\n",
    "for _ in range(1):\n",
    "\n",
    "    episode_reward = 0\n",
    "    for _ in range(1000):\n",
    "        env.render()\n",
    "\n",
    "        states, reward, done, _ = env.step(env.action_space.sample()) # take a random action\n",
    "        # ret = env.step(0) # take a random action\n",
    "\n",
    "        episode_reward += reward\n",
    "        \n",
    "        print(\"Reward = \", reward)\n",
    "        print(\"States = \", states)\n",
    "        \n",
    "        if done == True:\n",
    "            print(f\"AGENT IS DONE. TOTAL REWARD = {episode_reward}\")\n",
    "            rewards.append(episode_reward)\n",
    "            env.reset()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'register_env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-7ddda8eff3f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mregister_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ShipGym-v1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_creator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m pbt = PopulationBasedTraining(\n",
      "\u001b[0;31mNameError\u001b[0m: name 'register_env' is not defined"
     ]
    }
   ],
   "source": [
    "game_config = GameConfig\n",
    "game_config.FPS = 1000\n",
    "game_config.SPEED = 30\n",
    "game_config.BOUNDS = (1000, 1000)\n",
    "\n",
    "def env_creator(env_config):\n",
    "\n",
    "    env_config = EnvConfig\n",
    "    env = ShipEnv(game_config, env_config)\n",
    "\n",
    "    return env\n",
    "\n",
    "register_env(\"ShipGym-v1\", env_creator)\n",
    "\n",
    "pbt = PopulationBasedTraining(\n",
    "time_attr=\"time_total_s\",\n",
    "reward_attr=\"episode_reward_mean\",\n",
    "perturbation_interval=300, # 5 mins\n",
    "resample_probability=0.25,\n",
    "\n",
    "# Specifies the mutations of these hyperparams\n",
    "hyperparam_mutations={\n",
    "    \"lambda\": lambda: random.uniform(0.9, 1.0),\n",
    "    \"clip_param\": lambda: random.uniform(0.01, 0.5),\n",
    "    \"lr\": [1e-3, 5e-4, 1e-4, 5e-5, 1e-5],\n",
    "    \"num_sgd_iter\": lambda: random.randint(1, 30),\n",
    "    \"sgd_minibatch_size\": lambda: random.randint(128, 16384),\n",
    "    \"train_batch_size\": lambda: random.randint(2000, 160000),\n",
    "})\n",
    "\n",
    "ray.init()\n",
    "\n",
    "n_goals = 5\n",
    "reward_done = .9*n_goals\n",
    "\n",
    "run_experiments(\n",
    "{\n",
    "    \"pbt_ship_sim\": {\n",
    "        \"run\": \"PPO\",\n",
    "        \"env\": \"ShipGym-v1\",\n",
    "        \"num_samples\": 12, # Repeat the experiment this many times\n",
    "        \"checkpoint_at_end\" : True,\n",
    "        \"checkpoint_freq\" : 10,\n",
    "        \"config\": {\n",
    "            \"kl_coeff\": 1.0,\n",
    "            \"num_workers\": multiprocessing.cpu_count() - 1,\n",
    "            \"num_gpus\": 1,\n",
    "            \n",
    "            # These params are tuned from a fixed starting value.\n",
    "            \"lambda\": 0.95,\n",
    "            \"clip_param\": 0.2,\n",
    "            \"lr\" : 5.0e-4,\n",
    "            \"num_sgd_iter\":\n",
    "                lambda spec: random.choice([10, 20, 30]),\n",
    "            \"sgd_minibatch_size\":\n",
    "                lambda spec: random.choice([128, 512, 2048]),\n",
    "            \"train_batch_size\":\n",
    "                lambda spec: random.choice([10000, 20000, 40000])\n",
    "        },\n",
    "    },\n",
    "}, scheduler=pbt) # Reference the scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Visualisation\n",
    "\n",
    "rllib has very nice Tensorboard integration.\n",
    "\n",
    "http://185.165.71.107:6006/\n",
    "\n",
    "Insert TensorBoard image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-19-0b0dd02cf49b>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-19-0b0dd02cf49b>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    SHOW RESULT\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ship-sim-gym3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
