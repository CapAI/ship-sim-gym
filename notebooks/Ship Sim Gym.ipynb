{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ship Sim Gym\n",
    "\n",
    "Quick overview of training on a custom environment as it is slightly different from a normal one. \n",
    "\n",
    "Most cells contain code that is simply imported from corresponding py files. It is usually possible to call those scripts directly via their main hooks as well as long as you call them as modules `python -m ...`\n",
    "\n",
    "If you change something in the scripts themselves and would like to see the cell updated here as well, uncomment the first line in the cell that does the IPython magic called %load. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This makes sure we have access to the top level directory and so the containing modules\n",
    "import os\n",
    "import sys\n",
    "\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Ship Game\n",
    "\n",
    "This is a basic top down ship simulator built with pygame for the graphics and uses pymunk for physics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.4\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "Loading chipmunk for Linux (64bit) [/home/simons/.miniconda/envs/ship-sim-gym-0.0.1/lib/python3.6/site-packages/pymunk/libchipmunk.so]\n",
      "------------------------------\n",
      "SHIP GAME INITIALIZED\n",
      "DEBUG MODE =  True\n",
      "GAME SPEED =  1\n",
      "GAME FPS   =  30\n",
      "------------------------------ \n",
      "\n",
      "W pressed. \n",
      "W pressed. \n",
      "W pressed. \n",
      "W pressed. \n",
      "W pressed. \n",
      "W pressed. \n",
      "W pressed. \n",
      "A pressed.  -5\n",
      "W pressed. \n",
      "W pressed. \n",
      "W pressed. \n",
      "W pressed. \n",
      "W pressed. \n",
      "W pressed. \n",
      "W pressed. \n",
      "A pressed.  -10\n",
      "W pressed. \n",
      "W pressed. \n",
      "W pressed. \n",
      "W pressed. \n",
      "W pressed. \n",
      "W pressed. \n",
      "W pressed. \n",
      "W pressed. \n",
      "W pressed. \n",
      "W pressed. \n",
      "D pressed.  -5\n",
      "W pressed. \n",
      "W pressed. \n",
      "W pressed. \n",
      "W pressed. \n",
      "W pressed. \n",
      "W pressed. \n",
      "W pressed. \n",
      "W pressed. \n",
      "W pressed. \n",
      "W pressed. \n",
      "W pressed. \n",
      "W pressed. \n",
      "W pressed. \n",
      "W pressed. \n",
      "W pressed. \n",
      "D pressed.  0\n",
      "D pressed.  5\n",
      "D pressed.  10\n",
      "D pressed.  10\n",
      "D pressed.  10\n"
     ]
    }
   ],
   "source": [
    "# %load ../ship_gym/game.py\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pygame\n",
    "\n",
    "from pymunk import Vec2d, Transform\n",
    "import pymunk as pm\n",
    "import pymunk.pygame_util\n",
    "\n",
    "from ship_gym import game_map\n",
    "from ship_gym.config import GameConfig\n",
    "from ship_gym.models import GameObject, Ship, PolyEnv, LiDAR\n",
    "\n",
    "N_GOALS = 5\n",
    "DEFAULT_BOUNDS = (500, 500)\n",
    "\n",
    "\n",
    "class ShipGame(object):\n",
    "\n",
    "    ships = list()\n",
    "    goals = list()\n",
    "\n",
    "    frame_counter = 0\n",
    "    base_dt = 0.1\n",
    "    colliding = False\n",
    "    observe_mode = False\n",
    "    record = False\n",
    "\n",
    "    def __init__(self, game_config=None):\n",
    "\n",
    "        if game_config is None:\n",
    "            game_config = GameConfig\n",
    "\n",
    "        self.speed = game_config.SPEED\n",
    "        self.fps = game_config.FPS\n",
    "        self.bounds = game_config.BOUNDS\n",
    "        self.screen = pygame.display.set_mode(self.bounds)\n",
    "        self.clock = pygame.time.Clock()\n",
    "        self.goal_reached = False\n",
    "        self.colliding = False\n",
    "\n",
    "        self.debug_mode = game_config.DEBUG\n",
    "\n",
    "        pygame.init()\n",
    "        pygame.display.set_caption(\"Ship Sim Gym\")\n",
    "        pygame.key.set_repeat(10, 10)\n",
    "\n",
    "        print(\"-\"*30)\n",
    "        print(\"SHIP GAME INITIALIZED\")\n",
    "        print(\"DEBUG MODE = \", self.debug_mode)\n",
    "        print(\"GAME SPEED = \", self.speed)\n",
    "        print(\"GAME FPS   = \", self.fps)\n",
    "        print(\"-\"*30, \"\\n\")\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def gen_level(self):\n",
    "        \"\"\"\n",
    "        Generate a level on the fly by calling game map gen river poly function wrapping them in a GeoMap object\n",
    "        and adding the generated pymunk primitives (shapes and bodies) to the game space\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        poly = game_map.gen_river_poly(self.bounds)\n",
    "\n",
    "        self.level = PolyEnv(poly, self.bounds)\n",
    "\n",
    "        for body, shape in zip(self.level.bodies, self.level.shapes):\n",
    "            self.space.add(body, shape)\n",
    "\n",
    "    def invert_p(self, p):\n",
    "        \"\"\"Because in screen Y=0 is at the top or some shit like that \"\"\"\n",
    "        return Vec2d(p[0], self.bounds[1] - p[1])\n",
    "\n",
    "    def add_goal(self, x, y):\n",
    "        \"\"\"Add a ball to the given space at a random position \"\"\"\n",
    "        self.total_goals += 1\n",
    "\n",
    "        mass = 1\n",
    "        radius = 5\n",
    "        inertia = pm.moment_for_circle(mass, 0, radius, (0,0))\n",
    "        body = pm.Body(mass, inertia)\n",
    "\n",
    "        body.position = x, y\n",
    "        shape = pm.Circle(body, radius, (0,0))\n",
    "        shape.color = pygame.color.THECOLORS[\"green\"]\n",
    "        self.space.add(body, shape)\n",
    "        shape.collision_type = 2\n",
    "\n",
    "        goal = GameObject(body, shape)\n",
    "        self.goals.append(goal)\n",
    "\n",
    "        return goal\n",
    "\n",
    "    def add_player_ship(self, x, y, width, height, color):\n",
    "        \"\"\"\n",
    "        Call this after you have created the level!\n",
    "        Creates a new Ship instance and adds a shape and body to the pymunk space\n",
    "        :param self:\n",
    "        :param x:\n",
    "        :param y:\n",
    "        :param width:\n",
    "        :param height:\n",
    "        :param color:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        ship = Ship(x, y, width, height, color)\n",
    "        ship.add_lidar(self.level.shapes)\n",
    "\n",
    "        self.space.add(ship.body, ship.shape)\n",
    "\n",
    "        return ship\n",
    "\n",
    "    def add_ship(self, x, y, width, height, color):\n",
    "        \"\"\"\n",
    "        Creates a new Ship instance and adds a shape and body to the pymunk space\n",
    "        :param self:\n",
    "        :param x:\n",
    "        :param y:\n",
    "        :param width:\n",
    "        :param height:\n",
    "        :param color:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        ship = Ship(x, y, width, height, color)\n",
    "        self.space.add(ship.body, ship.shape)\n",
    "\n",
    "        return ship\n",
    "\n",
    "    def get_screen(self):\n",
    "        \"\"\"\n",
    "        Returns the game's screen space buffer as a 3D (color) array\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return pygame.surfarray.array3d(self.screen)\n",
    "\n",
    "    def handle_discrete_action(self, action):\n",
    "        \"\"\"\n",
    "        Handle discrete actions: It is possible to move forward, rotate left and right and do nothing.\n",
    "        Moving backwards is not possible, but is easy to add if needed. See the player definition in models for this.\n",
    "        :param action: integer value to indicate the action to take\n",
    "        \"\"\"\n",
    "        if action == 0:\n",
    "            self.player.move_forward()\n",
    "        elif action == 1:\n",
    "            self.player.rotate(-5)\n",
    "        elif action == 2:\n",
    "            self.player.rotate(+5)\n",
    "        elif action == 3:\n",
    "            pass\n",
    "\n",
    "    def handle_input(self):\n",
    "        \"\"\"\n",
    "        Maps key inputs to actions (via handle_discrete_action) and other utility functions such as quit\n",
    "        \"\"\"\n",
    "\n",
    "         # Handle key strokes\n",
    "        for event in pygame.event.get():\n",
    "\n",
    "            # print(event.key)\n",
    "            if event.type == pygame.QUIT:\n",
    "                sys.exit(0)\n",
    "\n",
    "            if event.type == pygame.KEYDOWN:\n",
    "                if event.key == pygame.K_ESCAPE or event.key == pygame.K_q:\n",
    "                    sys.exit(0)\n",
    "\n",
    "                elif event.key == pygame.K_w:\n",
    "                    self.handle_discrete_action(0)\n",
    "                    print(\"W pressed. \")\n",
    "                elif event.key == pygame.K_s:\n",
    "                    print(\"S pressed. Button not configured\")\n",
    "                    # self.handle_discrete_action(1)\n",
    "                elif event.key == pygame.K_a:\n",
    "                    self.handle_discrete_action(1)\n",
    "                    print(\"A pressed. \", self.player.rudder_angle)\n",
    "                elif event.key == pygame.K_d:\n",
    "                    self.handle_discrete_action(2)\n",
    "                    print(\"D pressed. \", self.player.rudder_angle)\n",
    "\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\"\n",
    "        The main update loop, resets certain event states, handles input, sensor routines and updates the game's\n",
    "        pymunk space\n",
    "        \"\"\"\n",
    "        self.colliding = False\n",
    "        self.goal_reached = False\n",
    "        self.handle_input()\n",
    "        self.player.query_sensors()\n",
    "        self.space.step(self.speed * self.base_dt)\n",
    "        self.clock.tick(self.fps)\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        The main render loop clears the screen and draws primitives if requested\n",
    "        \"\"\"\n",
    "        self.screen.fill((0, 0, 200))\n",
    "        if self.debug_mode:\n",
    "            options = pm.pygame_util.DrawOptions(self.screen)\n",
    "            options.flags = pymunk.SpaceDebugDrawOptions.DRAW_SHAPES\n",
    "            self.space.debug_draw(options)\n",
    "\n",
    "            res = self.player.lidar.query_results\n",
    "            for r in res:\n",
    "                if r is not None and r.shape is None:\n",
    "                    p = r.point\n",
    "                    p = self.invert_p(p)\n",
    "                    p = (round(p.x), round(p.y))\n",
    "\n",
    "                    # Green circle indicating the rays did not hot anything\n",
    "                    pygame.draw.circle(self.screen, (0, 255, 0), p, 10)\n",
    "                else:\n",
    "                    p = r.point\n",
    "                    p = self.invert_p(p)\n",
    "                    p = (round(p.x), round(p.y))\n",
    "\n",
    "                    # Red circle\n",
    "                    pygame.draw.circle(self.screen, (255, 0, 0), p, 10)\n",
    "\n",
    "        p = self.invert_p(self.player.position)\n",
    "\n",
    "        pygame.draw.circle(self.screen, (255, 255, 0), (round(p.x), round(p.y)), 10)\n",
    "        pygame.display.flip()\n",
    "\n",
    "        self.frame_counter += 1\n",
    "\n",
    "\n",
    "    def collide_ship(self, arbiter, space, data):\n",
    "        \"\"\"\n",
    "        Ship collision callback for when the player ship hits another ship. All params are ignored at this point\n",
    "        :param arbiter:\n",
    "        :param space:\n",
    "        :param data:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.colliding = True\n",
    "        return True\n",
    "\n",
    "    def collide_goal(self, arbiter, space, data):\n",
    "        \"\"\"\n",
    "        Ship collision callback for when the player ship hits a goal object. All params are ignored at this point\n",
    "        :param arbiter:\n",
    "        :param space:\n",
    "        :param data:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        shape = arbiter.shapes[1]\n",
    "        space.remove(shape, shape.body)\n",
    "\n",
    "        self.goal_reached = True\n",
    "        self.goals = [g for g in self.goals if g.body is not shape.body]\n",
    "\n",
    "        return False\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the game. Create the environment, the player and the goals\n",
    "        :param spawn_point:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.total_goals = 0\n",
    "        self.ships = list()\n",
    "        self.goals = list()\n",
    "        self.space = pm.Space()\n",
    "        self.space.damping = 0.4\n",
    "        self.create_environment()\n",
    "        self.gen_goal_path(N_GOALS)\n",
    "\n",
    "        spawn_point = Vec2d(self.bounds[0] / 2, 25)\n",
    "        self.player = self.add_player_ship(spawn_point.x, spawn_point.y, 2, 3, pygame.color.THECOLORS[\"white\"])\n",
    "        self.player.shape.collision_type = 0\n",
    "        self.setup_collision_handlers()\n",
    "\n",
    "    def add_default_traffic(self):\n",
    "        \"\"\"\n",
    "        Add some simple static traffic to the game\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.ships.append(self.add_ship(100, 200, 1, 1, pygame.color.THECOLORS[\"black\"]))\n",
    "        self.ships.append(self.add_ship(300, 200, 1.5, 2, pygame.color.THECOLORS[\"black\"]))\n",
    "        self.ships.append(self.add_ship(400, 350, 1, 3, pygame.color.THECOLORS[\"black\"]))\n",
    "\n",
    "    def setup_collision_handlers(self):\n",
    "        \"\"\"\n",
    "        Add collision handlers to the game space for goal and obstacle interactions.\n",
    "        \"\"\"\n",
    "        h = self.space.add_collision_handler(0, 1)\n",
    "        h.begin = self.collide_ship\n",
    "\n",
    "        goal_agent_col = self.space.add_collision_handler(0, 2)\n",
    "        goal_agent_col.begin = self.collide_goal\n",
    "\n",
    "        self.space.add_collision_handler(0, 3)\n",
    "\n",
    "    def gen_goal_path(self, n):\n",
    "        \"\"\"\n",
    "        Generate a path of goals by sampling somewhat randomly the coordinate space. To avoid complete randomness\n",
    "        where it is hard to even see a path, I kind of use a jittery approach where delta_y is computed according to\n",
    "        the game bounds and incremented, and randomly jittered. The X position is determined by taking a jittered\n",
    "        point close to the midline and doing a segment query to the left and right on environmental level shapes (see\n",
    "        ShapeFilter). These points are then used as extreme points between which the X value is determined according\n",
    "        to some tolerance value.\n",
    "        :param n: number of goals to generate\n",
    "        \"\"\"\n",
    "\n",
    "        y_delta = self.bounds[1] / (n+1)\n",
    "        x_middle = self.bounds[0] / 2\n",
    "        x_jitter = 50\n",
    "        y_jitter = 20\n",
    "\n",
    "        tolerance = 60\n",
    "        filter = pymunk.ShapeFilter(mask=pymunk.ShapeFilter.ALL_MASKS ^ 0b1) # This has not been properly tested!\n",
    "\n",
    "        for i in range(1, n+1):\n",
    "            y = y_delta*i + random.randint(-y_jitter, y_jitter)\n",
    "            try:\n",
    "                left_ret = self.space.segment_query((self.bounds[0]/2, y), (0, y), 10, filter)[0]\n",
    "                right_ret = self.space.segment_query((self.bounds[0] / 2, y), (self.bounds[0], y), 10, filter)[0]\n",
    "\n",
    "                x = np.random.uniform(left_ret.point.x + tolerance, right_ret.point.x - tolerance)\n",
    "                self.add_goal(x, y)\n",
    "\n",
    "            except Exception as e:\n",
    "                x = x_middle * i + random.randint(-x_jitter, x_jitter)\n",
    "                self.add_goal(x, y)\n",
    "\n",
    "\n",
    "    def closest_goal(self):\n",
    "        \"\"\"\n",
    "        Return the goal with the smallest Euclidean distance to the player. Returns None if there are no goals left.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if len(self.goals):\n",
    "            min_goal = self.goals[0]\n",
    "            min_distance = min_goal.body.position.get_distance(self.player.body.position)\n",
    "            for goal in self.goals[1:]:\n",
    "\n",
    "                dist = goal.body.position.get_distance(self.player.body.position)\n",
    "                if dist < min_distance:\n",
    "                    min_distance = dist\n",
    "                    min_goal = goal\n",
    "\n",
    "            return min_goal\n",
    "        return None\n",
    "\n",
    "    def create_environment(self):\n",
    "        \"\"\"\n",
    "        The hook for creating the environment. Replace the call for gen_level\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.gen_level()\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    import os\n",
    "\n",
    "    cwd = os.getcwd()\n",
    "    gc = GameConfig\n",
    "    gc.SPEED = 1\n",
    "    gc.FPS = 30\n",
    "    gc.DEBUG = True\n",
    "\n",
    "    g = ShipGame()\n",
    "\n",
    "    while True:\n",
    "        g.update()\n",
    "        g.render()\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Gym Environment\n",
    "\n",
    "This creates a wrapper for the ship game I made\n",
    "\n",
    "The way you create an OpenAI gym is by defining a class that overrides the gym.Env class and override a few properties:\n",
    "\n",
    "```\n",
    "action_space\n",
    "observation_space\n",
    "```\n",
    "\n",
    "and a few functions:\n",
    "\n",
    "```\n",
    "step()\n",
    "reset()\n",
    "render()\n",
    "```\n",
    "\n",
    "The step function is the most imporant one: It takes an action parameter, a single digit for a discrete action space or a vector of values for continuous action spaces. In our case we use a single digit to indicate what action to take:\n",
    "\n",
    "- 0 = move forward\n",
    "- 1 = rotate thruster left\n",
    "- 2 = rotate thruster right\n",
    "\n",
    "After every step new states are returned, again this depends on your observation_space, but in our case the layout is as follows:\n",
    "\n",
    "`Px Py R Gx Gy L1 L2 ... Ln`\n",
    "\n",
    "Where\n",
    "\n",
    "- P is the player position\n",
    "- A is the player angle\n",
    "- R is the rudder angle\n",
    "- G is the nearest goal position\n",
    "- L are the lidar values\n",
    "- N is the number of rays lidar uses\n",
    "\n",
    "Find more information in the code below. It's important to note that the states are critically important. If you are making your own environment you would want to test them well. If the states are not configured well it's not weird the AI doesnt learn anything. \n",
    "\n",
    "`reset()` resets the environment like respawning the player ship and generating a new environment. It also returns the states as observed at reset.\n",
    "\n",
    "`render()` is used mostly to get additional debug information. You could for example not render the game to screen, but use a `render()` function explicitly to render only when you want to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ship_gym'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d22f5384e8c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseeding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mship_gym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mShipGame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mDEFAULT_STATE_VAL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ship_gym'"
     ]
    }
   ],
   "source": [
    "# %load ../ship_gym/ship_env.py\n",
    "import sys\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from gym import Env\n",
    "from gym.spaces import Box, Discrete\n",
    "from gym.utils import seeding\n",
    "\n",
    "from ship_gym.game import ShipGame\n",
    "\n",
    "DEFAULT_STATE_VAL = -1\n",
    "STEP_PENALTY = -0.01\n",
    "\n",
    "\n",
    "class ShipEnv(Env):\n",
    "\n",
    "    metadata = {'render.modes': ['human', 'rgb_array']}\n",
    "    action_space = Discrete(3)\n",
    "    reward_range = (-1, 1)\n",
    "\n",
    "    # TODO: Derive the discrete actions\n",
    "    def __init__(self, game_config, env_config):\n",
    "\n",
    "        # TODO: Should add some basic sanity checks (max_steps > 0 etc.)\n",
    "        self.last_action = None\n",
    "        self.reward = 0\n",
    "        self.cumulative_reward = 0\n",
    "        self.step_count = 0\n",
    "        self.env_config = env_config\n",
    "\n",
    "        self.game = ShipGame(game_config)\n",
    "        self.episodes_count = -1 # Because the first reset will increment it to 0\n",
    "\n",
    "\n",
    "        \"\"\"P is the player position\n",
    "        A is the player angle\n",
    "        R is the rudder angle\n",
    "        G is the nearest goal position\n",
    "        L are the lidar values\n",
    "        N is the number of rays lidar uses\n",
    "        \"\"\"\n",
    "        self.n_states = 2 + 1 + 1 + 2 + self.game.player.lidar.n_beams\n",
    "        self.states_history = self.n_states * self.env_config.HISTORY_SIZE\n",
    "\n",
    "        if self.env_config.HISTORY_SIZE < 1:\n",
    "            raise ValueError(\"history_size must be greater than zero\")\n",
    "        self.observation_space = Box(low=0, high=max(self.game.bounds), shape=(self.states_history,), dtype=np.uint8)\n",
    "\n",
    "        # print(\" *** SHIP-GYM INITIALIZED *** \")\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        \"\"\"\n",
    "        Seed numpy random generator\n",
    "        :param seed: the seed to use\n",
    "        \"\"\"\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        # Important to actually seed it!!! I thought above would work but it's not enough\n",
    "        np.random.seed(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def determine_reward(self):\n",
    "        \"\"\"\n",
    "        Determines the reward of the current timestep\n",
    "        \"\"\"\n",
    "        if self.game.colliding:\n",
    "            self.reward = -1.0\n",
    "        if self.game.goal_reached:\n",
    "            self.reward = 1.0\n",
    "\n",
    "        # TODO: Code duplication with is_done()\n",
    "        elif self.game.player.x < 0 or self.game.player.x > self.game.bounds[0]:\n",
    "            self.reward = -1\n",
    "        elif self.game.player.y < 0 or self.game.player.y > self.game.bounds[1]:\n",
    "            self.reward = -1\n",
    "        else:\n",
    "            self.reward = STEP_PENALTY  # Small penalty\n",
    "\n",
    "    def __add_states(self):\n",
    "        '''\n",
    "        Push back some new state information for the current timestep onto the FIFO queue for all history timesteps\n",
    "        it keeps track of.\n",
    "\n",
    "        Layout of a single time step state is like this:\n",
    "\n",
    "        Px Py R Gx Gy L1 L2 ... Ln\n",
    "\n",
    "        Where\n",
    "        P is the player position\n",
    "        A is the player angle\n",
    "        R is the rudder angle\n",
    "        G is the nearest goal position\n",
    "        L are the lidar values\n",
    "        N is the number of rays lidar uses\n",
    "\n",
    "        :return: the complete history buffer of states extended with the most recent one\n",
    "        '''\n",
    "\n",
    "        states = self.n_states * [-1]\n",
    "\n",
    "        # Myself\n",
    "        goal = self.game.closest_goal()\n",
    "        goal_pos = [-1, -1]\n",
    "        player = self.game.player\n",
    "\n",
    "        if goal:\n",
    "            goal_pos = [goal.body.position.x, goal.body.position.y]\n",
    "        states[:6] = [player.x, player.y, player.rudder_angle, player.body.angle, goal_pos[0], goal_pos[1]]\n",
    "\n",
    "        lidar_vals = self.game.player.lidar.vals\n",
    "\n",
    "        states[6:] = lidar_vals\n",
    "        self.states.extend(states)\n",
    "\n",
    "    def is_done(self):\n",
    "        \"\"\"\n",
    "        Determines whether the episode has finished based on collisions and goals reached.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if self.game.colliding:\n",
    "            return True\n",
    "        elif len(self.game.goals) == 0:\n",
    "            return True\n",
    "\n",
    "        player = self.game.player\n",
    "        if player.x < 0 or player.x > self.game.bounds[0]:\n",
    "            return True\n",
    "        elif player.y < 0 or player.y > self.game.bounds[1]:\n",
    "            return True\n",
    "\n",
    "        if self.step_count >= self.env_config.MAX_STEPS:\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "\n",
    "        :param action:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        assert self.action_space.contains(action), \"%r (%s) invalid\" % (action, type(action))\n",
    "        self.game.handle_discrete_action(action)\n",
    "\n",
    "        self.game.update()\n",
    "        self.game.render()\n",
    "\n",
    "        self.determine_reward()\n",
    "        self.cumulative_reward += self.reward\n",
    "        self.__add_states()\n",
    "        self.step_count += 1\n",
    "\n",
    "        done = self.is_done()\n",
    "\n",
    "        return np.array(self.states), self.reward, done, {}\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        \"\"\"\n",
    "        Display additional debug information about the state of the environment here. You might also render actual images\n",
    "        or videos from the game's frame buffer (not currently implemented)\n",
    "        :param mode: the display mode, currently ignored but might be use to visualise different ways\n",
    "        :param close: Whether to close the environment. Also ignored in the current version\n",
    "        \"\"\"\n",
    "        out = sys.stdout\n",
    "\n",
    "        if self.last_action is not None:\n",
    "            out.write(f'action={self.last_action}, cumm_reward={self.cumulative_reward}')\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        self.game.reset()\n",
    "        self.last_action = None\n",
    "        self.reward = 0\n",
    "        self.cumulative_reward = 0\n",
    "        self.step_count = 0\n",
    "        self.episodes_count += 1\n",
    "\n",
    "        # Setup states\n",
    "        n = self.n_states * self.env_config.HISTORY_SIZE\n",
    "        self.states = deque([DEFAULT_STATE_VAL] * n, maxlen=n)\n",
    "        self.__add_states()\n",
    "\n",
    "        return np.array(self.states)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Agent\n",
    "\n",
    "A simple baseline to compare against is a random agent. Note that we run it at a slow speed so you can easily observe the behavior. Try changing FPS and SPEED for funsies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../train/random.py\n",
    "import random\n",
    "import time\n",
    "\n",
    "import gym\n",
    "import ship_gym\n",
    "import numpy as np\n",
    "from ship_gym.ship_env import ShipEnv\n",
    "\n",
    "from ship_gym.config import EnvConfig, GameConfig\n",
    "\n",
    "gc = GameConfig\n",
    "gc.DEBUG = True\n",
    "gc.SPEED = 1\n",
    "gc.FPS = 30\n",
    "\n",
    "env = ShipEnv(game_config=gc, env_config=EnvConfig)\n",
    "\n",
    "env.reset()\n",
    "cont = True\n",
    "\n",
    "for _ in range(1000):\n",
    "\n",
    "    total_reward = 0\n",
    "    for _ in range(100):\n",
    "        env.render()\n",
    "\n",
    "        ret = env.step(env.action_space.sample())  # take a random action\n",
    "\n",
    "        print(ret)\n",
    "        total_reward += ret[1]\n",
    "        if ret[2] == True:\n",
    "            print(f\"AGENT IS DONE. TOTAL REWARD = {total_reward}\")\n",
    "            env.reset()\n",
    "            break\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stable Baselines\n",
    "\n",
    "[Stable baselines](https://github.com/hill-a/stable-baselines) is a nice fork of Open AI baselines. It's better documented, has nicer code and better more intuitive methods of training. Still it's pretty low level when compared to rllib, but that also gives you some flexibility\n",
    "\n",
    "Read more about it in the [docs](https://stable-baselines.readthedocs.io)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %load ../train/stable_baselines/ppo.py\n",
    "import multiprocessing\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from baselines.results_plotter import ts2xy\n",
    "from stable_baselines.bench import load_results, Monitor\n",
    "from stable_baselines.common import set_global_seeds\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines import PPO2, ACER\n",
    "\n",
    "from ship_gym.config import EnvConfig, GameConfig\n",
    "from ship_gym.ship_env import ShipEnv\n",
    "from datetime import datetime\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "log_dir = \"logs/learning\"\n",
    "model_dir = \"models\"\n",
    "log_step_interval = 10000\n",
    "\n",
    "def callback(_locals, _globals):\n",
    "    \"\"\"\n",
    "    Callback called at each step (for DQN an others) or after n steps (see ACER or PPO2)\n",
    "    :param _locals: (dict)\n",
    "    :param _globals: (dict)\n",
    "    \"\"\"\n",
    "    global n_steps, best_mean_reward, t_last\n",
    "    # Print stats every 1000 calls\n",
    "    t = time.time()\n",
    "\n",
    "    if (n_steps + 1) % log_step_interval == 0:\n",
    "\n",
    "        # Evaluate policy performance\n",
    "        x, y = ts2xy(load_results(log_dir), 'timesteps')\n",
    "        if len(x) > 0:\n",
    "            mean_reward = np.mean(y[-100:])\n",
    "            print(x[-1], 'timesteps')\n",
    "            print(\n",
    "                \"Best mean reward: {:.2f} - Last mean reward per episode: {:.2f}\".format(best_mean_reward, mean_reward))\n",
    "\n",
    "            # New best model, you could save the agent here\n",
    "            if mean_reward > best_mean_reward:\n",
    "                best_mean_reward = mean_reward\n",
    "                # Example for saving best model\n",
    "                print(\"Saving new best model\")\n",
    "                _locals['self'].save(log_dir + 'best_model.pkl')\n",
    "    n_steps += 1\n",
    "    return False\n",
    "\n",
    "def make_env():\n",
    "        \"\"\"\n",
    "        Utility function for multiprocessed env.\n",
    "\n",
    "        :param n_goals:\n",
    "        :param env_id: (str) the environment ID\n",
    "        :param num_env: (int) the number of environment you wish to have in subprocesses\n",
    "        :param seed: (int) the inital seed for RNG\n",
    "        :param rank: (int) index of the subprocess\n",
    "        \"\"\"\n",
    "\n",
    "        game_config = GameConfig\n",
    "        game_config.FPS = 1000\n",
    "        game_config.SPEED = 30\n",
    "        game_config.DEBUG = True # This will render more primitives to make it more observable for humans (you), although this is not necessary for training and incurs a small performance hit\n",
    "        game_config.BOUNDS = (1000, 1000)\n",
    "\n",
    "        def _init():\n",
    "            env_config = EnvConfig\n",
    "            env = ShipEnv(game_config, env_config)\n",
    "            return env\n",
    "\n",
    "        return _init\n",
    "\n",
    "def get_model_path(lr):\n",
    "    return os.path.join(model_dir, f\"result_lr{lr}\")\n",
    "\n",
    "\n",
    "tb_root_dir = os.path.join(log_dir, \"tb\", str(int(time.time())))\n",
    "\n",
    "def train(model_cls, tid, env, lr, steps):\n",
    "\n",
    "    start_t = time.time()\n",
    "    tb_dir = os.path.join(tb_root_dir, f\"{tid}_{model_cls.__name__}\")\n",
    "    model = model_cls(MlpPolicy, env, learning_rate=lr, verbose=1, tensorboard_log=tb_dir)\n",
    "\n",
    "    model.learn(total_timesteps=steps, log_interval=10000)\n",
    "\n",
    "    end_t = time.time()\n",
    "    elapsed = end_t - start_t\n",
    "\n",
    "    print(f\"Trained {steps} steps in {elapsed} seconds\")\n",
    "    print(f\"Speed = {steps / (elapsed / 60)} steps/min\")\n",
    "    print()\n",
    "\n",
    "    path = get_model_path(lr)\n",
    "    model.save(path)\n",
    "\n",
    "def main():\n",
    "\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    np.set_printoptions(suppress=True)\n",
    "\n",
    "    ''' SET UP YOUR (HYPER)PARAMETERS HERE'''\n",
    "\n",
    "    def make_lr_func(start, stop):\n",
    "\n",
    "        def lr_func(frac):\n",
    "            return start + (stop - start) * (1-frac)\n",
    "\n",
    "        return lr_func\n",
    "\n",
    "    lrs = [1.0e-3, 1.0e-4, 1.0e-5]\n",
    "    lrs = [make_lr_func(s, 0) for s in lrs]\n",
    "\n",
    "    # Setting it to the number of CPU's you have is usually optimal\n",
    "    num_cpu = multiprocessing.cpu_count()\n",
    "    env = SubprocVecEnv([make_env() for i in range(num_cpu)])\n",
    "\n",
    "    i = 0\n",
    "    steps = int(1e6)\n",
    "\n",
    "    for lr in lrs:\n",
    "        print(f\"\"\"\n",
    "Started training at {datetime.now()}\n",
    "------------------------------------------------\n",
    "Training Steps \t:\\t {steps}\n",
    "Learning rate   :\\t {lr} \n",
    "            \"\"\")\n",
    "\n",
    "        i += 1\n",
    "        train(PPO2, i, env, lr, steps)\n",
    "        # train(ACER, env, n_goals, 0, lr, steps)\n",
    "\n",
    "    print(\"*\" * 30)\n",
    "    print(\" \"*10,\"     DONE!     \", \" \"*10)\n",
    "    print(\" \", datetime.now(), \" \")\n",
    "    print(\"*\" * 30)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RLLib\n",
    "\n",
    "RLlib is one of the few frameworks that I found that actually attempts to solve the scaling up problem effectively. Stable baselines has some basic parallelization, but doesn't really deal with the issue of hyper-parameter search or scaling in terms of hardware / clusters.\n",
    "\n",
    "RLLib is built on top of ray, which is a high-performance execution engine (sounds awesome right?). In fact this is used for their Tune framework as well which is a hyper parameter search and experimentation tool on top of good old deep learning implementations (but extended for RL as well)\n",
    "\n",
    "I will show you one neat setup of RLLib with Tune that uses something called population based training. It basically runs experimments multiple times and then evaluates how well it performs, randomly resampling from a start distribution (**exploration**) or slighly perturbing a top performer (**exploitation**).\n",
    "\n",
    "Because RL is very hyper-parameter sensitive you can imagine that such a tool would be very helpful. There is a lot more to explore, go visit their docs for [more](https://ray.readthedocs.io/en/latest/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../train/rllib/pbt \n",
    "import random\n",
    "\n",
    "import ray\n",
    "from ray.tune import run_experiments, register_env\n",
    "\n",
    "from ray.tune.schedulers import PopulationBasedTraining\n",
    "\n",
    "from ship_gym.config import GameConfig, EnvConfig\n",
    "from ship_gym.ship_env import ShipEnv\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    game_config = GameConfig\n",
    "    game_config.FPS = 1000\n",
    "    game_config.SPEED = 30\n",
    "    game_config.BOUNDS = (1000, 1000)\n",
    "\n",
    "    def env_creator(env_config):\n",
    "\n",
    "        env_config = EnvConfig\n",
    "        env = ShipEnv(game_config, env_config)\n",
    "\n",
    "        return env\n",
    "\n",
    "    register_env(\"ShipGym-v1\", env_creator)\n",
    "\n",
    "    pbt = PopulationBasedTraining(\n",
    "        time_attr=\"time_total_s\",\n",
    "        reward_attr=\"episode_reward_mean\",\n",
    "        perturbation_interval=600, # 10 mins\n",
    "        resample_probability=0.33, # Should we start with a new config or modify a good performing one?\n",
    "\n",
    "        # Specifies the mutations of these hyperparams\n",
    "        hyperparam_mutations={\n",
    "            \"lambda\": lambda: random.uniform(0.9, 1.0),\n",
    "            \"clip_param\": lambda: random.uniform(0.01, 0.5),\n",
    "            \"lr\": [1e-3, 5e-4, 1e-4, 5e-5, 1e-5],\n",
    "            \"num_sgd_iter\": lambda: random.randint(1, 30),\n",
    "            \"sgd_minibatch_size\": lambda: random.randint(128, 16384),\n",
    "            \"train_batch_size\": lambda: random.randint(2000, 160000),\n",
    "        })\n",
    "\n",
    "    ray.init()\n",
    "    \n",
    "    run_experiments(\n",
    "        {\n",
    "            \"pbt_ship_sim_v2\": {\n",
    "                \"run\": \"PPO\",\n",
    "                \"env\": \"ShipGym-v1\",\n",
    "                \"num_samples\": 120, # Repeat the experiment this many times\n",
    "                \"checkpoint_at_end\" : True,\n",
    "                \"checkpoint_freq\" : 2,\n",
    "                \"config\": {\n",
    "                    \"kl_coeff\": 1.0,\n",
    "                    \"num_workers\": multiprocessing.cpu_count() - 1,\n",
    "                    \"num_gpus\": 1,\n",
    "                    # These params are tuned from a fixed starting value.\n",
    "                    \"lambda\": 0.95,\n",
    "                    \"clip_param\": 0.2,\n",
    "                    \"lr\" : 5.0e-4,\n",
    "\n",
    "                    # These params start off randomly drawn from a set.\n",
    "                    \"num_sgd_iter\":\n",
    "                        lambda spec: random.choice([10, 20, 30]),\n",
    "                    \"sgd_minibatch_size\":\n",
    "                        lambda spec: random.choice([128, 512, 2048]),\n",
    "                    \"train_batch_size\":\n",
    "                        lambda spec: random.choice([10000, 20000, 40000])\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "        scheduler=pbt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Progress\n",
    "\n",
    "Ray / RLlib records progress stats by default in a folder in the home directory called ray_results. It has some CSVs and if enabled is also able to save TensorBoard logs.\n",
    "\n",
    "Run `tensorboard --logdir ~/ray_results` on Mac and Linux, not sure about Windows and go to the URL:\n",
    " \n",
    "[http://localhost:6006/](http://localhost:6006/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO with RLlib\n",
    "\n",
    "It's interesting to train a PPO agent similar to how we trained it with stable-baselines. Everything is a lot easier and much more declarative: simply stating an expirement definition is enough. If you'd prefer there are ways of training it in a more procedural way similar to stable-baselines.\n",
    "\n",
    "The below example uses the declarative method. Note that I use an lr schedule below to decrease learning rate linearly over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../train/rllib/ppo.py\n",
    "import ray\n",
    "\n",
    "from ray import tune\n",
    "\n",
    "from ship_gym.config import GameConfig, EnvConfig\n",
    "from ship_gym.ship_env import ShipEnv\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    game_config = GameConfig\n",
    "    game_config.FPS = 100000\n",
    "    game_config.SPEED = 40\n",
    "    game_config.BOUNDS = (1000, 1000)\n",
    "\n",
    "    ray.init(num_gpus=1)\n",
    "\n",
    "    def env_creator():\n",
    "\n",
    "        env_config = EnvConfig\n",
    "        env = ShipEnv(game_config, env_config)\n",
    "\n",
    "        return env\n",
    "\n",
    "    experiments = {\n",
    "        \"shipgym_best\": {\n",
    "            \"run\": \"PPO\",\n",
    "            \"stop\": {\n",
    "                \"time_total_s\": 12 * 60 * 60 # 12 hours\n",
    "            },\n",
    "            \"env\": \"ship-gym-v1\",\n",
    "            \"config\": {\n",
    "                \"num_gpus\": 1,\n",
    "                \"num_workers\" : multiprocessing.cpu_count() - 1,\n",
    "                \"num_sgd_iter\" :  10,\n",
    "                \"sgd_minibatch_size\" : 2048,\n",
    "                \"train_batch_size\" : 10000,\n",
    "                \"lr_schedule\" : [[0, 0.001], [5e6, 0.0001], [1e7, 0.00001]]\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "    tune.register_env(\"ship-gym-v1\", env_creator)\n",
    "    tune.run_experiments(experiments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
